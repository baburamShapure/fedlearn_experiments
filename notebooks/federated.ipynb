{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.7.6 64-bit ('base': conda)",
   "display_name": "Python 3.7.6 64-bit ('base': conda)",
   "metadata": {
    "interpreter": {
     "hash": "33bb8adafb896594f8adda556f073b48cf8ae5908869505a35cb09ad41d6d274"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Training a Federated Model "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.optim as optim \n",
    "import torch.nn.functional as F \n",
    "import gc\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import tqdm\n",
    "import sklearn.metrics as metrics\n",
    "from common.dataset import *\n",
    "from common.models import * \n",
    "from common.utils import * \n",
    "import logging \n",
    "import argparse\n",
    "import datetime as dt \n",
    "import random \n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(keys)*len(weights) - can get expensive\n",
    "def average_weights(w):\n",
    "    w_avg = copy.deepcopy(w[0])\n",
    "    for k in w_avg.keys():\n",
    "        for i in range(1, len(w)):\n",
    "            w_avg[k] += w[i][k]\n",
    "        w_avg[k] = torch.div(w_avg[k], len(w))\n",
    "    return w_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATADIR = 'fl_data\\HHAR'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i']"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "FL_AGENTS = os.listdir(DATADIR)\n",
    "FL_AGENTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "FL_SAMPLE = 0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_model = FFN(48, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 33/33 [00:45<00:00,  1.38s/it]\n",
      "100%|██████████| 33/33 [00:48<00:00,  1.46s/it]\n",
      "100%|██████████| 33/33 [00:42<00:00,  1.30s/it]\n",
      "100%|██████████| 33/33 [00:42<00:00,  1.28s/it]\n",
      "100%|██████████| 33/33 [00:43<00:00,  1.32s/it]\n",
      "100%|██████████| 33/33 [00:43<00:00,  1.31s/it]\n",
      "100%|██████████| 33/33 [00:42<00:00,  1.30s/it]\n",
      "100%|██████████| 33/33 [00:42<00:00,  1.28s/it]\n",
      "100%|██████████| 33/33 [00:42<00:00,  1.28s/it]\n",
      "100%|██████████| 33/33 [00:42<00:00,  1.29s/it]\n",
      "100%|██████████| 30/30 [00:38<00:00,  1.27s/it]\n",
      "100%|██████████| 30/30 [00:38<00:00,  1.27s/it]\n",
      "100%|██████████| 30/30 [00:37<00:00,  1.25s/it]\n",
      "100%|██████████| 30/30 [00:37<00:00,  1.25s/it]\n",
      "100%|██████████| 30/30 [00:20<00:00,  1.48it/s]\n",
      "100%|██████████| 30/30 [00:20<00:00,  1.45it/s]\n",
      "100%|██████████| 30/30 [00:20<00:00,  1.47it/s]\n",
      "100%|██████████| 30/30 [00:26<00:00,  1.12it/s]\n",
      "100%|██████████| 30/30 [00:23<00:00,  1.27it/s]\n",
      "100%|██████████| 30/30 [00:22<00:00,  1.31it/s]\n",
      "100%|██████████| 6/6 [00:04<00:00,  1.35it/s]\n",
      "100%|██████████| 6/6 [00:04<00:00,  1.40it/s]\n",
      "100%|██████████| 6/6 [00:05<00:00,  1.15it/s]\n",
      "100%|██████████| 6/6 [00:04<00:00,  1.39it/s]\n",
      "100%|██████████| 6/6 [00:04<00:00,  1.39it/s]\n",
      "100%|██████████| 6/6 [00:04<00:00,  1.30it/s]\n",
      "100%|██████████| 6/6 [00:04<00:00,  1.35it/s]\n",
      "100%|██████████| 6/6 [00:04<00:00,  1.22it/s]\n",
      "100%|██████████| 6/6 [00:04<00:00,  1.31it/s]\n",
      "100%|██████████| 6/6 [00:05<00:00,  1.20it/s]\n",
      "Round: 0, Agent: a\n",
      "[0.345703125, 0.3290203327171904]\n",
      "Round: 0, Agent: b\n",
      "[0.767578125, 0.77001953125, 0.7058823529411765]\n",
      "Round: 0, Agent: c\n",
      "[0.5029296875, 0.5112293144208038]\n",
      "Round: 0, Agent: d\n",
      "[0.6787109375, 0.6641221374045801]\n",
      "Round: 0, Agent: e\n",
      "[0.56201171875, 0.5689834024896265]\n",
      "Round: 0, Agent: f\n",
      "[0.7001953125, 0.6872800402212167]\n",
      "Round: 0, Agent: g\n",
      "[0.62982689747004]\n",
      "Round: 0, Agent: h\n",
      "[0.44293614881850174]\n",
      "Round: 0, Agent: i\n",
      "[0.2303370786516854]\n"
     ]
    }
   ],
   "source": [
    "for each_round in range(1): \n",
    "    agents_to_train = random.choices(FL_AGENTS, k= int(FL_SAMPLE * len(FL_AGENTS)))\n",
    "    model_list = []\n",
    "    for each_agent in agents_to_train: \n",
    "        # read the data. \n",
    "        test, train = [pd.read_csv(os.path.join(DATADIR, each_agent, i)) for i in os.listdir(os.path.join(DATADIR, each_agent))]\n",
    "        train = train.fillna(0)\n",
    "        test = test.fillna(0)\n",
    "        trainData, testData = HARData(train), HARData(test)\n",
    "        trainLoader, testLoader = getDataLoader(trainData, testData)\n",
    "        loss = nn.CrossEntropyLoss()\n",
    "        model = copy.deepcopy(global_model)\n",
    "        optimizer = optim.Adam(model.parameters())\n",
    "        model.train()\n",
    "        for epoch in range(10): \n",
    "            # train each epoch. \n",
    "            for i, (x, y) in tqdm.tqdm(enumerate(trainLoader), total =len(trainLoader)): \n",
    "                yhat = model(x)\n",
    "                batch_loss = loss(yhat, y)\n",
    "                optimizer.zero_grad()\n",
    "                batch_loss.backward()\n",
    "                optimizer.step()\n",
    "            # for i, (x, y) in enumerate(testLoader): \n",
    "        model_list.append(model.state_dict())\n",
    "    # average weight at end of round. \n",
    "    avg_weights = average_weights(model_list)\n",
    "    global_model.load_state_dict(avg_weights)\n",
    "    # run tests on every agent's test data. \n",
    "    for each_agent in FL_AGENTS: \n",
    "        test, train = [pd.read_csv(os.path.join(DATADIR, each_agent, i)) for i in os.listdir(os.path.join(DATADIR, each_agent))]\n",
    "        train = train.fillna(0)\n",
    "        test = test.fillna(0)\n",
    "        trainData, testData = HARData(train), HARData(test)\n",
    "        trainLoader, testLoader = getDataLoader(trainData, testData)\n",
    "        print('Round: {0}, Agent: {1}'.format(each_round, each_agent))\n",
    "        print(get_accuracy(global_model, testLoader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}